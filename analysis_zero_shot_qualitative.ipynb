{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:42:45.469862Z",
     "start_time": "2024-10-09T07:42:42.426928Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.load_results import *\n",
    "from utils.plot_helpers import *\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('default')\n",
    "import torch\n",
    "from utils.analysis_from_interaction import *\n",
    "from language_analysis_local import TopographicSimilarityConceptLevel, encode_target_concepts_for_topsim\n",
    "import os\n",
    "if not os.path.exists('analysis'):\n",
    "    os.makedirs('analysis')\n",
    "#import plotly.express as px\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utilities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def objects_to_concepts(sender_input):\n",
    "    \"\"\"reconstruct concepts from objects in interaction\"\"\"\n",
    "    n_targets = int(sender_input.shape[1]/2)\n",
    "    # get target objects and fixed vectors to re-construct concepts\n",
    "    target_objects = sender_input[:, :n_targets]\n",
    "    target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "    # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "    (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "    concepts = list(zip(objects, fixed))\n",
    "    return concepts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T07:56:18.649262Z",
     "start_time": "2024-10-09T07:56:18.644438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def retrieve_messages(interaction):\n",
    "    \"\"\"retrieve messages from interaction\"\"\"\n",
    "    messages = interaction.message.argmax(dim=-1)\n",
    "    messages = [msg.tolist() for msg in messages]\n",
    "    return messages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T07:56:19.441215Z",
     "start_time": "2024-10-09T07:56:19.437450Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def count_symbols(messages):\n",
    "    \"\"\"counts symbols in messages\"\"\"\n",
    "    all_symbols = [symbol for message in messages for symbol in message]\n",
    "    symbol_counts = Counter(all_symbols)\n",
    "    return symbol_counts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T07:56:20.054421Z",
     "start_time": "2024-10-09T07:56:20.051657Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def get_unique_message_set(messages):\n",
    "    \"\"\"returns unique messages as a set ready for set operations\"\"\"\n",
    "    return set(tuple(message) for message in messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T07:56:20.702705Z",
     "start_time": "2024-10-09T07:56:20.699185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def get_unique_concept_set(concepts):\n",
    "    \"\"\"returns unique concepts\"\"\"\n",
    "    concept_tuples = []\n",
    "    for objects, fixed in concepts:\n",
    "        tuple_objects = []\n",
    "        for object in objects:\n",
    "            tuple_objects.append(tuple(object))\n",
    "        tuple_objects = tuple(tuple_objects)\n",
    "        tuple_concept = (tuple_objects, tuple(fixed))\n",
    "        concept_tuples.append(tuple_concept)\n",
    "    tuple(concept_tuples)\n",
    "    unique_concepts = set(concept_tuples)\n",
    "    return unique_concepts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T07:56:21.783204Z",
     "start_time": "2024-10-09T07:56:21.780023Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configurations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:47:47.514984Z",
     "start_time": "2024-10-09T08:47:47.511436Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = ['(3,4)', '(3,8)', '(3,16)', '(4,4)', '(4,8)', '(5,4)']\n",
    "n_values = [4, 8, 16, 4, 8, 4]\n",
    "n_attributes = [3, 3, 3, 4, 4, 5]\n",
    "n_epochs = 300\n",
    "n_datasets = len(datasets)\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "datasets = ['(3,4)', '(3,8)']\n",
    "n_values = [4, 8]\n",
    "n_attributes = [3, 3]\n",
    "n_epochs = 300\n",
    "n_datasets = len(datasets)\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3' for d in datasets]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T08:40:06.257383Z",
     "start_time": "2024-10-09T08:40:06.254886Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:47:59.777736Z",
     "start_time": "2024-10-09T11:47:59.775770Z"
    }
   },
   "outputs": [],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "zero_shot = True # whether zero-shot simulations are evaluated\n",
    "zero_shot_test = 'generic' # 'generic' or 'specific'\n",
    "test_interactions = True # whether scores should be calculated on test interactions (only with zero shot)\n",
    "test_as = 'test_sampled_unscaled' # 'test' or 'test_sampled_unscaled' or 'test_unscaled' or 'test_fine' \n",
    "setting = \"\"\n",
    "if context_unaware:\n",
    "    setting = setting + 'context_unaware'\n",
    "else:\n",
    "    setting = setting + 'standard'\n",
    "if zero_shot:\n",
    "    setting = setting + '/zero_shot/' + zero_shot_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine vocab size and message reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:49:45.162832Z",
     "start_time": "2024-10-09T11:48:02.093673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,4)\n",
      "1 novel messages used for the 12 novel concepts\n",
      "4 novel messages used for the 12 novel concepts\n",
      "6 novel messages used for the 12 novel concepts\n",
      "0 novel messages used for the 12 novel concepts\n",
      "1 novel messages used for the 12 novel concepts\n",
      "(3,8)\n",
      "5 novel messages used for the 24 novel concepts\n",
      "2 novel messages used for the 24 novel concepts\n",
      "2 novel messages used for the 24 novel concepts\n",
      "5 novel messages used for the 24 novel concepts\n",
      "5 novel messages used for the 24 novel concepts\n",
      "(3,16)\n",
      "14 novel messages used for the 48 novel concepts\n",
      "6 novel messages used for the 48 novel concepts\n",
      "12 novel messages used for the 48 novel concepts\n",
      "13 novel messages used for the 48 novel concepts\n",
      "11 novel messages used for the 48 novel concepts\n",
      "(4,4)\n",
      "1 novel messages used for the 16 novel concepts\n",
      "3 novel messages used for the 16 novel concepts\n",
      "4 novel messages used for the 16 novel concepts\n",
      "8 novel messages used for the 16 novel concepts\n",
      "5 novel messages used for the 16 novel concepts\n",
      "(4,8)\n",
      "1 novel messages used for the 32 novel concepts\n",
      "5 novel messages used for the 32 novel concepts\n",
      "1 novel messages used for the 32 novel concepts\n",
      "0 novel messages used for the 32 novel concepts\n",
      "3 novel messages used for the 32 novel concepts\n",
      "(5,4)\n",
      "0 novel messages used for the 20 novel concepts\n",
      "0 novel messages used for the 20 novel concepts\n",
      "0 novel messages used for the 20 novel concepts\n",
      "0 novel messages used for the 20 novel concepts\n",
      "0 novel messages used for the 20 novel concepts\n"
     ]
    }
   ],
   "source": [
    "# go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    print(d)\n",
    "    for run in range(5):\n",
    "        path_to_run = paths[i] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_test = (path_to_run + 'interactions/' + str(test_as) +'/epoch_0/interaction_gpu0')\n",
    "        interaction_train = torch.load(path_to_interaction_train)\n",
    "        interaction_val = torch.load(path_to_interaction_val)\n",
    "        interaction_test = torch.load(path_to_interaction_test)\n",
    "        \n",
    "        concepts_train = objects_to_concepts(interaction_train.sender_input)\n",
    "        concepts_val = objects_to_concepts(interaction_val.sender_input)\n",
    "        concepts_test = objects_to_concepts(interaction_test.sender_input)\n",
    "        \n",
    "        messages_train = retrieve_messages(interaction_train)\n",
    "        messages_val = retrieve_messages(interaction_val)\n",
    "        messages_test = retrieve_messages(interaction_test)\n",
    "    \n",
    "        symbol_counts_train = count_symbols(messages_train)\n",
    "        symbol_counts_val = count_symbols(messages_val)\n",
    "        symbol_counts_test = count_symbols(messages_test)\n",
    "        symbol_counts = [symbol_counts_train, symbol_counts_val, symbol_counts_test]\n",
    "        pickle.dump(symbol_counts, open(path_to_run + 'symbol_counts_' + str(test_as) + '.pkl', 'wb'))\n",
    "        \n",
    "        # consider train and validation messages together\n",
    "        messages_train_val = messages_train +  messages_val\n",
    "        # consider only unique messages\n",
    "        messages_train_val_unique = get_unique_message_set(messages_train_val)\n",
    "        #print(\"messages train val\", len(messages_train_val), len(messages_train_val_unique))\n",
    "        messages_test_unique = get_unique_message_set(messages_test)\n",
    "        #print(\"messages test\", len(messages_test), len(messages_test_unique))\n",
    "        # total messages\n",
    "        messages_total = messages_train_val +  messages_test\n",
    "        messages_total_unique = get_unique_message_set(messages_total)\n",
    "        \n",
    "        # concepts\n",
    "        concepts_train_unique = get_unique_concept_set(concepts_train)\n",
    "        concepts_val_unique = get_unique_concept_set(concepts_val)\n",
    "        concepts_test_unique = get_unique_concept_set(concepts_test)\n",
    "        #print(\"concepts\", len(concepts_test), len(concepts_test_unique))\n",
    "        concepts_total = concepts_train + concepts_val + concepts_test\n",
    "        concepts_total_unique = get_unique_concept_set(concepts_total)\n",
    "        num_of_concepts = [len(concepts_train_unique), len(concepts_val_unique), len(concepts_test_unique), len(concepts_total_unique), len(concepts_total)]\n",
    "        pickle.dump(num_of_concepts, open(path_to_run + 'num_of_concepts_' + str(test_as) + '.pkl', 'wb'))\n",
    "        \n",
    "        # messages reused in testing:\n",
    "        intersection = messages_train_val_unique & messages_test_unique\n",
    "        \n",
    "        # messages only used in training:\n",
    "        difference_train = messages_train_val_unique - messages_test_unique\n",
    "        \n",
    "        # messages only used in testing:\n",
    "        difference_test = messages_test_unique - messages_train_val_unique\n",
    "        print(len(difference_test), \"novel messages used for the\", len(concepts_test_unique), \"novel concepts\")\n",
    "        \n",
    "        message_reuse = [len(intersection), len(difference_train), len(difference_test), len(concepts_test_unique), (len(difference_test)/len(concepts_test_unique)), len(messages_test_unique)]\n",
    "        pickle.dump(message_reuse, open(path_to_run + 'message_reuse_' + str(test_as) + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "message_reuse_dict = {'intersection': [], 'difference train': [], 'difference test': [], 'concepts test unique': [], 'test ratio': [], 'messages test unique': [],\n",
    "                      'reuse rate': [], 'novelty rate': [], 'total ratio': []}\n",
    "for i, d in enumerate(datasets):\n",
    "    intersection, train_difference, test_difference, test_concepts, test_ratio, test_messages, reuse_rate, novelty_rate, total_ratio = [], [], [], [], [], [], [], [], []\n",
    "    for run in range(5):\n",
    "        path_to_run = paths[i] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "        message_reuse = pickle.load(open(path_to_run + 'message_reuse_' + str(test_as) + '.pkl', 'rb'))\n",
    "        intersection.append(message_reuse[0])\n",
    "        train_difference.append(message_reuse[1])\n",
    "        test_difference.append(message_reuse[2])\n",
    "        test_concepts.append(message_reuse[3])\n",
    "        test_ratio.append(message_reuse[4])\n",
    "        test_messages.append(message_reuse[5])\n",
    "        reuse_rate.append(message_reuse[0]/message_reuse[5])\n",
    "        novelty_rate.append(message_reuse[2]/message_reuse[5])\n",
    "        total_ratio.append(message_reuse[5]/message_reuse[3]) # test_messages / test_concepts (novel unique messages & concepts)\n",
    "    message_reuse_dict['intersection'].append(intersection)\n",
    "    message_reuse_dict['difference train'].append(train_difference)\n",
    "    message_reuse_dict['difference test'].append(test_difference)\n",
    "    message_reuse_dict['concepts test unique'].append(test_concepts)\n",
    "    message_reuse_dict['test ratio'].append(test_ratio)\n",
    "    message_reuse_dict['messages test unique'].append(test_messages)\n",
    "    message_reuse_dict['reuse rate'].append(reuse_rate)\n",
    "    message_reuse_dict['novelty rate'].append(novelty_rate)\n",
    "    message_reuse_dict['total ratio'].append(total_ratio)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T11:49:45.175397Z",
     "start_time": "2024-10-09T11:49:45.170428Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "{} & novel concepts & total unique messages & message-concept ratio &       reuse rate &     novelty rate \\\\\n",
      "\\midrule\n",
      "D(3,4)  &             12 &        11.8 $\\pm$ 0.4 &       0.98 $\\pm$ 0.03 &  0.80 $\\pm$ 0.19 &  0.20 $\\pm$ 0.19 \\\\\n",
      "D(3,8)  &             24 &        22.2 $\\pm$ 0.4 &       0.93 $\\pm$ 0.02 &  0.83 $\\pm$ 0.07 &  0.17 $\\pm$ 0.07 \\\\\n",
      "D(3,16) &             48 &        44.4 $\\pm$ 0.8 &       0.93 $\\pm$ 0.02 &  0.75 $\\pm$ 0.06 &  0.25 $\\pm$ 0.06 \\\\\n",
      "D(4,4)  &             16 &        15.6 $\\pm$ 0.8 &       0.97 $\\pm$ 0.05 &  0.74 $\\pm$ 0.14 &  0.26 $\\pm$ 0.14 \\\\\n",
      "D(4,8)  &             32 &        28.8 $\\pm$ 3.1 &       0.90 $\\pm$ 0.10 &  0.93 $\\pm$ 0.06 &  0.07 $\\pm$ 0.06 \\\\\n",
      "D(5,4)  &             20 &        19.6 $\\pm$ 0.5 &       0.98 $\\pm$ 0.02 &  1.00 $\\pm$ 0.00 &  0.00 $\\pm$ 0.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/03rlh8jd6nqbws0_xg9jh20w0000gq/T/ipykernel_9677/2396398592.py:35: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex_table = df.to_latex(index=True, escape=False)\n"
     ]
    }
   ],
   "source": [
    "message_reuse = [message_reuse_dict['concepts test unique'], message_reuse_dict['messages test unique'], message_reuse_dict['total ratio'], message_reuse_dict['reuse rate'], message_reuse_dict['novelty rate']]\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "mess_reuse_array = np.array(message_reuse)\n",
    "\n",
    "# Compute means and standard deviations over the five runs\n",
    "means = np.mean(mess_reuse_array, axis=-1)\n",
    "std_devs = np.std(mess_reuse_array, axis=-1)\n",
    "\n",
    "# Row names and column names\n",
    "row_names = [\"D(3,4)\", \"D(3,8)\", \"D(3,16)\", \"D(4,4)\", \"D(4,8)\", \"D(5,4)\"]\n",
    "col_names = [\"novel concepts\", \"total unique messages\", \"message-concept ratio\", \"reuse rate\",\"novelty rate\"]\n",
    "\n",
    "# Prepare the data for the DataFrames\n",
    "data = []\n",
    "\n",
    "# iterate over datasets\n",
    "for i in range(means.shape[1]):\n",
    "    row = []\n",
    "    # iterate over conditions\n",
    "    for j in range(means.shape[0]):\n",
    "        if j > 1:\n",
    "            formatted_value = f\"{means[j, i]:.2f} $\\\\pm$ {std_devs[j, i]:.2f}\"\n",
    "        elif j == 0:\n",
    "            formatted_value = f\"{int(means[j, i])}\"\n",
    "        else:\n",
    "            formatted_value = f\"{means[j, i]:.1f} $\\\\pm$ {std_devs[j, i]:.1f}\"\n",
    "        row.append(formatted_value)\n",
    "    data.append(row)\n",
    "\n",
    "# Create DataFrames\n",
    "df = pd.DataFrame(data, index=row_names, columns=col_names)\n",
    "\n",
    "# Convert DataFrames to LaTeX tables\n",
    "latex_table = df.to_latex(index=True, escape=False)\n",
    "\n",
    "print(latex_table)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T11:49:45.197504Z",
     "start_time": "2024-10-09T11:49:45.192818Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Symbol reuse\n",
    "Also in \"to generic\" condition, all symbols are reused during testing, i.e. they all encode relevant information. This is why a qualitative analysis of messages makes more sense."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def symbol_frequency(interaction, n_attributes, n_values, vocab_size, is_gumbel=True):\n",
    "    messages = interaction.message.argmax(dim=-1) if is_gumbel else interaction.message\n",
    "    messages = messages[:, :-1] # without EOS\n",
    "    sender_input = interaction.sender_input\n",
    "    n_objects = sender_input.shape[1]\n",
    "    n_targets = int(n_objects / 2)\n",
    "    # k_hots = sender_input[:, :-n_attributes]\n",
    "    # objects = k_hot_to_attributes(k_hots, n_values)\n",
    "    target_objects = sender_input[:, :n_targets]\n",
    "    target_objects = k_hot_to_attributes(target_objects, n_values)\n",
    "    # intentions = sender_input[:, -n_attributes:]  # (0=same, 1=any)\n",
    "    (objects, fixed) = retrieve_concepts_sampling(target_objects)\n",
    "\n",
    "    objects[fixed == 1] = np.nan\n",
    "\n",
    "    objects = objects\n",
    "    messages = messages\n",
    "    favorite_symbol = {}\n",
    "    mutual_information = {}\n",
    "    for att in range(n_attributes):\n",
    "        for val in range(n_values):\n",
    "            object_labels = (objects[:, att] == val).astype(int)\n",
    "            max_MI = 0\n",
    "            for symbol in range(vocab_size):\n",
    "                symbol_indices = np.argwhere(messages == symbol)[0]\n",
    "                symbol_labels = np.zeros(len(messages))\n",
    "                symbol_labels[symbol_indices] = 1\n",
    "                MI = normalized_mutual_info_score(symbol_labels, object_labels)\n",
    "                if MI > max_MI:\n",
    "                    max_MI = MI\n",
    "                    max_symbol = symbol\n",
    "            favorite_symbol[str(att) + str(val)] = max_symbol\n",
    "            mutual_information[str(att) + str(val)] = max_MI\n",
    "\n",
    "    return favorite_symbol, mutual_information"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T06:58:19.776172Z",
     "start_time": "2024-10-07T06:58:19.772518Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "zero_shot = True # whether zero-shot simulations are evaluated\n",
    "zero_shot_test = 'generic' # 'generic' or 'specific'\n",
    "test_interactions = True # whether scores should be calculated on test interactions (only with zero shot)\n",
    "setting = \"\"\n",
    "if context_unaware:\n",
    "    setting = setting + 'context_unaware'\n",
    "else:\n",
    "    setting = setting + 'standard'\n",
    "if zero_shot:\n",
    "    setting = setting + '/zero_shot/' + zero_shot_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T07:18:55.894107Z",
     "start_time": "2024-10-07T07:18:55.892767Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00': 11, '01': 11, '02': 11, '03': 11, '10': 2, '11': 12, '12': 7, '13': 2, '20': 8, '21': 8, '22': 8, '23': 8}\n",
      "torch.Size([60, 20, 12])\n",
      "3 [0, 1, 2] [3, 0, 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "sample for dataset (3,4) could not be generated",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [108], line 61\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28mprint\u001B[39m(df)\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;66;03m#df.to_csv('analysis/quali_' + str(d) + '_' + str(setting) + '_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv', index=False)\u001B[39;00m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;66;03m#print('saved ' + 'analysis/quali_' + str(d) + '_' + str(setting) + '_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv')\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 61\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msample for dataset \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(d) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m could not be generated\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: sample for dataset (3,4) could not be generated"
     ]
    }
   ],
   "source": [
    "for run in range(5):\n",
    "    path_to_run = paths[0] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "    path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "    path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "    path_to_interaction_test = (path_to_run + 'interactions/test/epoch_0/interaction_gpu0')\n",
    "    interaction_train = torch.load(path_to_interaction_train)\n",
    "    interaction_val = torch.load(path_to_interaction_val)\n",
    "    interaction_test = torch.load(path_to_interaction_test)\n",
    "    \n",
    "    # retrieve \"lexicon\" based on mutual information\n",
    "    # hard-code for D(3,4) for now\n",
    "    favorite_symbol, mutual_information = symbol_frequency(interaction_train, n_attributes=3, n_values=4, vocab_size=13)\n",
    "    print(favorite_symbol)\n",
    "\n",
    "    messages = interaction_test.message.argmax(dim=-1)\n",
    "    messages = [msg.tolist() for msg in messages]\n",
    "    sender_input = interaction_test.sender_input\n",
    "    print(sender_input.shape)\n",
    "    n_targets = int(sender_input.shape[1]/2)\n",
    "    # get target objects and fixed vectors to re-construct concepts\n",
    "    target_objects = sender_input[:, :n_targets]\n",
    "    target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "    # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "    (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "    concepts = list(zip(objects, fixed))\n",
    "\n",
    "    # get distractor objects to re-construct context conditions\n",
    "    distractor_objects = sender_input[:, n_targets:]\n",
    "    distractor_objects = k_hot_to_attributes(distractor_objects, n_values[i])\n",
    "    context_conds = retrieve_context_condition(objects, fixed, distractor_objects)\n",
    "\n",
    "    # get random qualitative samples\n",
    "    #fixed_index = random.randint(0, n_attributes[i]-1) # define a fixed index for the concept\n",
    "    #n_fixed = random.randint(1, n_attributes[i]) # how many fixed attributes?\n",
    "    n_fixed = 3\n",
    "    #fixed_indices = random.sample(range(0, n_attributes[i]), k=n_fixed) # select which attributes are fixed\n",
    "    fixed_indices = [0, 1, 2]\n",
    "    #fixed_value = random.randint(0, n_values[i]-1) # define a fixed value for this index\n",
    "    fixed_values = random.choices(range(0, n_values[i]), k=n_fixed)\n",
    "    fixed_values = [3, 0, 1]\n",
    "    print(n_fixed, fixed_indices, fixed_values)\n",
    "    #index_threshold = 20000 # optional: define some index threshold to make sure that examples are not taken from the beginning of training\n",
    "    # TODO: adapt this loop such that multiple indices can be fixed\n",
    "    all_for_this_concept = []\n",
    "    for idx, (t_objects, t_fixed) in enumerate(concepts):\n",
    "        #if sum(t_fixed) == 1 and t_fixed[fixed_index] == 1:# and idx > index_threshold:\n",
    "        if sum(t_fixed) == n_fixed and all(t_fixed[fixed_index] == 1 for fixed_index in fixed_indices):\n",
    "            for t_object in t_objects:\n",
    "                if all(t_object[fixed_index] == fixed_values[j] for j, fixed_index in enumerate(fixed_indices)):\n",
    "                    all_for_this_concept.append((idx, t_object, t_fixed, context_conds[idx], messages[idx]))\n",
    "                    fixed = t_fixed\n",
    "    if len(all_for_this_concept) > 0:\n",
    "        #sample = random.sample(all_for_this_concept, 20)\n",
    "        sample = all_for_this_concept\n",
    "        column_names = ['game_nr', 'object', 'fixed indices', 'context condition', 'message']\n",
    "        df = pd.DataFrame(sample, columns=column_names)\n",
    "        print(df)\n",
    "        #df.to_csv('analysis/quali_' + str(d) + '_' + str(setting) + '_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv', index=False)\n",
    "        #print('saved ' + 'analysis/quali_' + str(d) + '_' + str(setting) + '_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv')\n",
    "    else:\n",
    "        raise ValueError(\"sample for dataset \" + str(d) + \" could not be generated\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T07:18:57.329921Z",
     "start_time": "2024-10-07T07:18:57.186314Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
